This project builds on the work completed in previous micro-projects:

Micro-Project 1: Data preprocessing (cleaning, handling missing values, feature transformation)

Micro-Project 2: Data visualization (exploring distributions, correlations, and patterns)

Micro-Project 3: Implementation of several classical ML models (Logistic Regression, SVM, Decision Tree, Random Forest, KNN).


Micro-Project 4 extends this pipeline by:

1.	Introducing Deep Learning models (Baseline MLP, Enhanced MLP, LSTM).

2.	Adding Gradient Boosting and Naive Bayes, which were not included in Micro-Project 3 but later tested and delivered strong results.

 Results Summary
 
-Gradient Boosting was the top performer (Accuracy = 0.91, F1 = 0.92, ROC AUC = 0.95).

-Naive Bayes proved to be a simple yet competitive baseline (Accuracy = 0.89, F1 = 0.90, AUC = 0.94).

-Enhanced MLP achieved balanced performance (Accuracy = 0.87, F1 = 0.88, AUC = 0.94).

-LSTM lagged behind (Accuracy = 0.80, F1 = 0.81, AUC = 0.90).

  Key Findings:
  
-Ensemble-based ML methods, particularly Gradient Boosting, remain the most effective for this dataset.

-Naive Bayes provided strong results despite its simplicity.

-DL models (MLP variants) show promise but do not outperform the strongest ML models.

-LSTM is less suitable in this context, misclassifying too many positive cases.

â¸»
